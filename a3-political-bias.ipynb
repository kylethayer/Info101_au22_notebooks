{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb9e2fa-5f32-4ddc-bf9c-5256d8e5076a",
   "metadata": {},
   "source": [
    "# A3: Political Bias\n",
    "In this assignment, you will be trying to calcualate the average political bias and reliability of tweets in tweet searches.\n",
    "\n",
    "We have already provided you code that goes through all the tweets in a search and finds the political bias and reliability of url web addresses mentioned in the tweets. You will need to add loop variables to calculate these averages (see chapter 8 practice and demos).\n",
    "\n",
    "Then you will need to try your code with some different searches, and then you will answer some reflection questions on them.\n",
    "\n",
    "First, we'll do our normal twitter login steps (though we'll not use fake-tweepy for this assignment, only real twitter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef2034-dcde-40b6-9329-b446a0e931f5",
   "metadata": {},
   "source": [
    "## Tweepy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b9f36-4fe8-4852-adf5-1135f70c7e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7be23-dfc0-43e6-a049-acd686dc3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bot_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b3e41-1c0e-4fad-bc4d-9c1fd0c28a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into tweepy\n",
    "client = tweepy.Client(\n",
    "    bearer_token=bot_keys.bearer_token,\n",
    "    consumer_key=bot_keys.consumer_key, consumer_secret=bot_keys.consumer_secret,                   \n",
    "    access_token=bot_keys.access_token, access_token_secret=bot_keys.access_token_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dbd35-52bf-4b5b-943d-3485ef7bad6a",
   "metadata": {},
   "source": [
    "## Load Bias and Reliability Info\n",
    "Our measure of media website bias and reliability is based on the Media Bias Chart 9.0 (https://adfontesmedia.com/static-mbc)\n",
    "\n",
    "I have made a file with some of the sites shown on that chart. I made my own simplification based on the grid in the graphic with bias ranging from -4 to +4, and reliability from -4 to +2 as follows:\n",
    "![Media bias chart with grid, showing the range labels](./imgs/bias_chart_divisions.jpg)\n",
    "\n",
    "\n",
    "Note: If you want to look more at the file, it is saved as `media_info.csv`(a comma separated variable file).\n",
    "- Each row of text is the info for one media site. For example:\n",
    "`wsws.org,-3,-1`\n",
    "Means that the site wsws.org has a bias of -3 (Hyper-Partisan Left) and a reliability of -1 (Opinion OR High Variation in Reliability). \n",
    "- You can open the by right-clicking media_info.csv and selecting Open With -> Editor. You can then modify entries, or add new ones for more sites (like from the interactive media bias chart here: https://adfontesmedia.com/interactive-media-bias-chart/). Then save it and rerun this code and the code below.\n",
    "\n",
    "The code below loads the bias and reliability info and makes three things that the code will use later:\n",
    "- a `find_matching_site(url)` function, which given a url to an article or something, tries to find the site it's from (e.g., \"https://www.npr.org/sections/codeswitch/\" is from the site \"npr.org\"\n",
    "- `media_bias_lookup` a dictionary that when given a site (like \"npr.org\"), finds the bias for the site (for npr.org: -1)\n",
    "- `media_reliability_lookup` a dictionary that when given a site (like \"npr.org\"), finds the reliability for the site (for npr.org: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789def50-4d8a-493b-a8fd-e4c19f677011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pandas code library, which helps us use the csv file\n",
    "import pandas as pd\n",
    "\n",
    "# load the media_info.csv file\n",
    "media_info_df = pd.read_csv('media_info.csv')\n",
    "\n",
    "# get a list of the sites we have information for in the media_info.csv\n",
    "media_sites = media_info_df['site']\n",
    "\n",
    "def find_matching_site(url):\n",
    "    for site in media_sites:\n",
    "        if site in url:\n",
    "            return site   \n",
    "\n",
    "# make a lookup dictionary, where they key is the site, and the value is the bias for that site\n",
    "media_bias_lookup = {m_info['site']: m_info['bias'] for i, m_info in media_info_df.iterrows()}\n",
    "\n",
    "# make a lookup dictionary, where they key is the site, and the value is the reliability for that site\n",
    "media_reliability_lookup = {m_info['site']: m_info['reliability'] for i, m_info in media_info_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c2e28-ce05-4414-a6c7-d6431cc2744b",
   "metadata": {},
   "source": [
    "## TODO: Modify the code below (Run Search)\n",
    "The code below runs a search on twitter, and the loops through each tweet. For each of the tweets it then loops through all the url web addresses in that tweet, and if it can find a match for the site it is from, calculates the bias and reliability for that site.\n",
    "\n",
    "__Your job__ is to add loop variables to the code to calculate the number of matched urls and then the total bias and total reliability for those urls. Then you can use that at the end to calculate the average bias and average reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e03b95-f18f-47d9-855a-b8a824d8a57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The search query starts with the search term and then has options to\n",
    "#   make sure it isn't a retweet and that there are url links in the tweet that aren't for twitter.com\n",
    "query = 'senate   -is:retweet has:links -url:twitter.com'\n",
    "\n",
    "max_results = 20 # You can go up to 100 tweets at time in your search\n",
    "    \n",
    "tweet_search_results = client.search_recent_tweets(query=query, max_results=max_results, tweet_fields=[\"entities\"])\n",
    "\n",
    "\n",
    "#### TODO: make three loop variables here: ####\n",
    "#   number_matched_urls\n",
    "#   total_url_bias\n",
    "#   total_url_reliability\n",
    "\n",
    "\n",
    "# go through all the tweets\n",
    "for tweet in tweet_search_results.data:\n",
    "    \n",
    "    print(tweet.text)\n",
    "\n",
    "    # go through all the urls in the tweet\n",
    "    for url in tweet.entities['urls']:\n",
    "        # get the \"unwound\" url, so it isn't a shortened url like t.co/Drh9zcdEQz\n",
    "        unwound_url =url['unwound_url']\n",
    "\n",
    "        print(\"url: \" + url['unwound_url'])\n",
    "\n",
    "        # try to find the url site from the sites in our media_info.csv file\n",
    "        matching_site = find_matching_site(unwound_url)\n",
    "\n",
    "        # if we found the matching site, then we have info for it\n",
    "        if(matching_site):\n",
    "            \n",
    "            # look up the bias and reliability for the site the url is from\n",
    "            url_bias = media_bias_lookup[matching_site]\n",
    "            url_reliability = media_reliability_lookup[matching_site]\n",
    "\n",
    "            #### TODO: Update the three loop variables here #### \n",
    "\n",
    "            print(\"  bias: \" + str(url_bias))\n",
    "            print(\"  reliability: \" + str(url_reliability))\n",
    "        else:\n",
    "            # We didn't have info on this site\n",
    "            print(\"**did not recognize site!\")\n",
    "\n",
    "    print()\n",
    "        \n",
    "\n",
    "####  TODO: Use the loop variables to calculate the total number of urls #### \n",
    "#  we cold measure and then the average bias and reliability for those\n",
    "# Then display them with the print statements below\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Total number of urls we could measure: \")\n",
    "print(\"Average bias: \")\n",
    "print(\"Average reliability: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be133008-425f-4f8d-bdd1-70ad89b80ed9",
   "metadata": {},
   "source": [
    "## Reflection tasks\n",
    "\n",
    "Once you get the code above working and finding an average bias and reliability, modify the search (replace the word \"senate\") to try at least three more searches, looking over the tweets and the final results, and answer the questions below.\n",
    "\n",
    "Note: For searches, you can search for political terms, or news terms, or even phrases associated with different communities, like \"Soros\", \"cult\", \"#censorship\", etc.\n",
    "\n",
    "1. What additional searches did you run (at least 3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c473e9-5b16-4edd-9777-8bbff17a1c95",
   "metadata": {},
   "source": [
    "TODO: Answer the question here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86be7b-ecb5-4c96-b62a-2daa34e613f2",
   "metadata": {},
   "source": [
    "2. When doing those searches, what were your observations about the calculations of media bias and reliability? (For example: were there a lot of urls that you didn't measure? Do you feel like the final calculated bias and reliability match the search results?). Answer with at least 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcffa15-14b2-4e6c-9028-7720d9c4b348",
   "metadata": {},
   "source": [
    "TODO: Answer the question here with at least 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806f464-3fa7-4df4-94ea-263604df64b9",
   "metadata": {},
   "source": [
    "3. If you could redesign the Media Bias Chart, what would you want to do (e.g., add some other dimension besides just bias/responsibility, change how it is evaluated, add more news sources, consider different countries)? Answer with at least 3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb476ec9-4306-4169-8afb-8f9141f550e3",
   "metadata": {},
   "source": [
    "TODO: Answer the question here with at least 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2d1c9-68d8-420c-8181-2bca8f974f43",
   "metadata": {},
   "source": [
    "4. What might a social media companies or advertizers (including political campaigns) want to do with information on a users' political views and susceptibility to consipracy theories? Answer with at least 3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaffa22-0536-4a79-a5f4-f70f9662e43e",
   "metadata": {},
   "source": [
    "TODO: Answer the question here with at least 3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db14d6f-0dad-42ec-a8f8-3959ec040e72",
   "metadata": {},
   "source": [
    "5. Choose two ethics frameworks and use the frameworks to consider the different uses of the media bias and reliability information. Answer with at least 6 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a34598-ec1d-4dd7-94ee-469170bfd176",
   "metadata": {},
   "source": [
    "TODO: Answer the question here with at least 3 sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
